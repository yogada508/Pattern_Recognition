{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3: Decision Tree, AdaBoost and Random Forest\n",
    "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset and test the performance with testing data\n",
    "\n",
    "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling sklearn.tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "The dataset is the Heart Disease Data Set from UCI Machine Learning Repository. It is a binary classifiation dataset, the label is stored in `target` column. **Please note that there exist categorical features which need to be [one-hot encoding](https://www.datacamp.com/community/tutorials/categorical-data) before fit into your model!**\n",
    "See follow links for more information\n",
    "https://archive.ics.uci.edu/ml/datasets/heart+Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
    "df = pd.read_csv(file_url)\n",
    "\n",
    "train_idx = np.load('train_idx.npy')\n",
    "test_idx = np.load('test_idx.npy')\n",
    "\n",
    "train_df = df.iloc[train_idx]\n",
    "test_df = df.iloc[test_idx]\n",
    "\n",
    "# remove the noise data\n",
    "train_df = train_df[train_df[\"cp\"].isin([1, 2, 3, 4])]\n",
    "train_df = train_df[train_df[\"thal\"].isin([\"normal\", \"fixed\", \"reversible\"])]\n",
    "test_df = test_df[test_df[\"cp\"].isin([1, 2, 3, 4])]\n",
    "test_df = test_df[test_df[\"thal\"].isin([\"normal\", \"fixed\", \"reversible\"])]\n",
    "\n",
    "# one hot encoding for \"thal\" field\n",
    "thal_encoded = pd.get_dummies(train_df[\"thal\"], prefix=\"thal\")\n",
    "train_df = train_df.drop(columns=\"thal\")\n",
    "train_df = pd.concat([train_df.iloc[:, 0:-1], thal_encoded, train_df.iloc[:, -1]], axis=1)\n",
    "thal_encoded = pd.get_dummies(test_df[\"thal\"], prefix=\"thal\")\n",
    "test_df = test_df.drop(columns=\"thal\")\n",
    "test_df = pd.concat([test_df.iloc[:, 0:-1], thal_encoded, test_df.iloc[:, -1]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal_fixed</th>\n",
       "      <th>thal_normal</th>\n",
       "      <th>thal_reversible</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>112</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>138</td>\n",
       "      <td>294</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>130</td>\n",
       "      <td>322</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>120</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>105</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "264   44    1   4       112   290    0        2      153      0      0.0   \n",
       "140   62    0   4       138   294    1        0      106      0      1.9   \n",
       "104   70    1   4       130   322    0        2      109      0      2.4   \n",
       "290   49    1   3       120   188    0        0      139      0      2.0   \n",
       "262   41    0   2       105   198    0        0      168      0      0.0   \n",
       "\n",
       "     slope  ca  thal_fixed  thal_normal  thal_reversible  target  \n",
       "264      1   1           0            1                0       1  \n",
       "140      2   3           0            1                0       1  \n",
       "104      2   3           0            1                0       0  \n",
       "290      2   3           0            0                1       1  \n",
       "262      1   1           0            1                0       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.iloc[:, 0:-1]#.to_numpy()\n",
    "y_train = train_df.iloc[:, -1]#.to_numpy()\n",
    "X_test = test_df.iloc[:, 0:-1]#.to_numpy()\n",
    "y_test = test_df.iloc[:, -1]#.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
       "       'exang', 'oldpeak', 'slope', 'ca', 'thal_fixed', 'thal_normal',\n",
       "       'thal_reversible'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns\n",
    "#y_train[a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(sequence, sample_weight=None):\n",
    "    information = 0\n",
    "\n",
    "    if sample_weight is not None:\n",
    "        # get sum of sample_weight\n",
    "        weight_sum = sum(sample_weight)\n",
    "        values = np.unique(sequence)\n",
    "\n",
    "        # get the information of each class\n",
    "        for value in values:\n",
    "            idx = [idx for idx, x in enumerate(sequence) if x==value]\n",
    "            count = np.sum(sample_weight[idx])\n",
    "            information += (count / weight_sum)**2\n",
    "\n",
    "    else:\n",
    "        values, counts = np.unique(sequence, return_counts=True)\n",
    "        for count in counts:\n",
    "            information += (count / len(sequence))**2\n",
    "\n",
    "    return 1 - information\n",
    "\n",
    "def entropy(sequence, sample_weight=None):\n",
    "    information = 0\n",
    "\n",
    "    if sample_weight is not None:\n",
    "        # get sum of sample_weight\n",
    "        weight_sum = sum(sample_weight)\n",
    "        values = np.unique(sequence)\n",
    "\n",
    "        # get the information of each class\n",
    "        for value in values:\n",
    "            idx = [idx for idx, x in enumerate(sequence) if x==value]\n",
    "            count = np.sum(sample_weight[idx])\n",
    "            information += -(count / weight_sum) * np.log2(count / weight_sum)\n",
    "\n",
    "    else:\n",
    "        values, counts = np.unique(sequence, return_counts=True)\n",
    "        for count in counts:\n",
    "            information += -(count / len(sequence)) * np.log2(count / len(sequence))\n",
    "\n",
    "    return information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = class 1,\n",
    "# 2 = class 2\n",
    "data = np.array([1,2,1,1,1,1,2,2,1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini of data is  0.4628099173553719\n"
     ]
    }
   ],
   "source": [
    "print(\"Gini of data is \", gini(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of data is  0.9456603046006401\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of data is \", entropy(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the test data. You should implement two arguments for the Decision Tree algorithm\n",
    "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
    "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, predicted_result, depth=None):\n",
    "        self.predicted_result = predicted_result\n",
    "        self.attribute = ''\n",
    "        self.threshold = 0\n",
    "        self.left_node = None\n",
    "        self.right_node = None\n",
    "        self.depth = depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_depth=None):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def split_attribute(self, X, y):\n",
    "        best_attribute = None\n",
    "        best_threshold = None\n",
    "\n",
    "        # calculate the information before split\n",
    "        if self.criterion == 'gini':\n",
    "            min_info = gini(y.to_numpy(), self.sample_weight)\n",
    "        else:\n",
    "            min_info = entropy(y.to_numpy(), self.sample_weight)\n",
    "\n",
    "        # iterate each attribute in training data\n",
    "        for attribute in X.columns:\n",
    "            data = pd.concat([X, y], axis=1)\n",
    "            sorted_data = data.sort_values(by=[attribute])\n",
    "            threshold = np.unique(sorted_data[attribute])\n",
    "            left_weight = None\n",
    "            right_weight = None\n",
    "\n",
    "            if self.sample_weight is not None:\n",
    "                data_with_weight = X.copy()\n",
    "                data_with_weight[\"weight\"] = self.sample_weight\n",
    "                data_with_weight = data_with_weight.sort_values(by=[attribute])\n",
    "\n",
    "            for i in range(1, len(threshold)):\n",
    "                # split data by threshold\n",
    "                left_data = sorted_data[sorted_data[attribute] < threshold[i]]\n",
    "                right_data = sorted_data[sorted_data[attribute] >= threshold[i]]\n",
    "                left_probability = len(left_data) / len(sorted_data)\n",
    "                right_probability = len(right_data) / len(sorted_data)\n",
    "\n",
    "                # if sample_weight is assigned, calculate probability of each class with sample weight \n",
    "                if self.sample_weight is not None:\n",
    "                    left_weight_data = data_with_weight[\"weight\"][left_data.index]\n",
    "                    left_weight = np.array(left_weight_data)\n",
    "                    left_probability = left_weight_data.sum() / data_with_weight[\"weight\"].sum()\n",
    "                    \n",
    "                    right_weight_data = data_with_weight[\"weight\"][right_data.index]\n",
    "                    right_weight = np.array(right_weight_data)\n",
    "                    right_probability = right_weight_data.sum() / data_with_weight[\"weight\"].sum()\n",
    "\n",
    "\n",
    "                # get the information of left branch and right branch\n",
    "                if self.criterion == 'gini':\n",
    "                    left_info = gini(left_data.iloc[:, -1], left_weight)\n",
    "                    right_info = gini(right_data.iloc[:, -1], right_weight)\n",
    "\n",
    "                else:\n",
    "                    left_info = entropy(left_data.iloc[:, -1], left_weight)\n",
    "                    right_info = entropy(right_data.iloc[:, -1], right_weight)\n",
    "\n",
    "                total_info = left_probability*left_info + right_probability*right_info\n",
    "                \n",
    "                # update the minimum information after split\n",
    "                if total_info < min_info:\n",
    "                    min_info = total_info\n",
    "                    best_attribute = attribute\n",
    "                    best_threshold = threshold[i]\n",
    "        \n",
    "        return best_attribute, best_threshold\n",
    "\n",
    "    def generate_tree(self, X, y, depth=0):\n",
    "        # assign the most frequent value in y to the current node's predicted result\n",
    "        predicated_result = y.mode()[0]\n",
    "        node = Node(predicated_result, depth)\n",
    "\n",
    "        if self.max_depth is None or node.depth < self.max_depth:\n",
    "            attribute, threshold = self.split_attribute(X, y)\n",
    "\n",
    "            # split the node\n",
    "            if attribute and threshold:\n",
    "                self.feature_importance[X.columns.get_loc(attribute)] += 1\n",
    "\n",
    "                X_left = X[X[attribute] < threshold]\n",
    "                y_left = y[X_left.index]\n",
    "                X_right = X[X[attribute] >= threshold]\n",
    "                y_right = y[X_right.index]\n",
    "\n",
    "                node.attribute = attribute\n",
    "                node.threshold = threshold\n",
    "\n",
    "                # recursively bulid the subtree\n",
    "                node.left_node = self.generate_tree(X_left, y_left, depth+1)\n",
    "                node.right_node = self.generate_tree(X_right, y_right, depth+1)\n",
    "        \n",
    "        return node\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        self.sample_weight = sample_weight\n",
    "        self.feature_importance = np.zeros(len(X.columns))\n",
    "        self.root_node = self.generate_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "\n",
    "        # predict the result row by row\n",
    "        for i in range(len(X)):\n",
    "            node = self.root_node\n",
    "\n",
    "            # stop when the current node is leaf-node\n",
    "            while node.left_node:\n",
    "                if X.iloc[i][node.attribute] < node.threshold:\n",
    "                    node = node.left_node\n",
    "                else:\n",
    "                    node = node.right_node\n",
    "\n",
    "            predictions.append(node.predicted_result)\n",
    "\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using `criterion=gini`, showing the accuracy score of test data by `max_depth=3` and `max_depth=10`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion=gini, max_depth=3, accuracy: 0.78\n",
      "criterion=gini, max_depth=10, accuracy: 0.69\n"
     ]
    }
   ],
   "source": [
    "clf_depth3.fit(X_train, y_train)\n",
    "print(\"criterion=gini, max_depth=3, accuracy:\", accuracy_score(clf_depth3.predict(X_test), y_test))\n",
    "\n",
    "clf_depth10.fit(X_train, y_train)\n",
    "print(\"criterion=gini, max_depth=10, accuracy:\", accuracy_score(clf_depth10.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using `max_depth=3`, showing the accuracy score of test data by `criterion=gini` and `criterion=entropy`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion=gini, max_depth=3, accuracy: 0.78\n",
      "criterion=entropy, max_depth=3, accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "clf_gini.fit(X_train, y_train)\n",
    "print(\"criterion=gini, max_depth=3, accuracy:\", accuracy_score(clf_gini.predict(X_test), y_test))\n",
    "\n",
    "clf_entropy.fit(X_train, y_train)\n",
    "print(\"criterion=entropy, max_depth=3, accuracy:\", accuracy_score(clf_entropy.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: All of your accuracy scores should over **0.7**\n",
    "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
    "- Hint: You can use the recursive method to build the nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "\n",
    "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
    "\n",
    "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 15 artists>"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Feature importance (Q2.1)\")\n",
    "plt.barh(X_train.columns, clf_depth10.feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n",
    "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "class AdaBoost():\n",
    "    def __init__(self, n_estimators):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.clfs = []\n",
    "        self.alpha = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize weights\n",
    "        n_train = len(X)\n",
    "        w = np.ones(n_train) / n_train\n",
    "        pred_train = [np.zeros(n_train)]\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            clf_1tree = DecisionTree(criterion=\"entropy\", max_depth=1)\n",
    "            #clf_1tree = DecisionTreeClassifier(max_depth = 1)\n",
    "            self.clfs.append(clf_1tree)\n",
    "            self.clfs[i].fit(X, y, sample_weight=w)\n",
    "            pred_train_i = self.clfs[i].predict(X)\n",
    "\n",
    "            # calculate the error from misclassified case\n",
    "            miss = [int(x) for x in (pred_train_i != y)]\n",
    "            error = np.dot(w,miss)\n",
    "\n",
    "            # Equivalent with 1/-1 to update weights\n",
    "            # -1 for predict error, 1 for predict correct\n",
    "            miss2 = [-1 if x==1 else 1 for x in miss]\n",
    "\n",
    "            # update alpha and data weight\n",
    "            alpha = 0.5 * np.log((1 - error) / error)\n",
    "            self.alpha.append(alpha)\n",
    "\n",
    "            w = np.multiply(w, np.exp([-x * self.alpha[i] for x in miss2]))\n",
    "\n",
    "            w = w / np.sum(w)\n",
    "\n",
    "            #pred_train = [sum(x) for x in zip(pred_train, [x * self.alpha[i] for x in pred])]\n",
    "\n",
    "            print(error, alpha, np.sum(w))\n",
    "            #print(len(miss), np.sum(miss))\n",
    "\n",
    "    def predict(self, X):\n",
    "        predicted_matrix = np.array([clf.predict(X) for clf in self.clfs])\n",
    "        result = np.zeros(predicted_matrix.shape)\n",
    "        \n",
    "        for i in range(predicted_matrix.shape[0]):\n",
    "            x = np.array([x if x==1 else -1 for x in predicted_matrix[i]])\n",
    "            result[i] = x * self.alpha[i]\n",
    "\n",
    "        result = np.sign(np.sum(result, axis=0))\n",
    "        final_result = np.array([1 if x==1 else 0 for x in result])\n",
    "\n",
    "        return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm([0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2373737373737374 0.5835661175524327 1.0\n",
      "0.27405946174439905 0.4870615058658215 0.9999999999999997\n",
      "0.261809101015504 0.5182934228764402 1.0\n",
      "0.37646955072254856 0.252280215291787 1.0\n",
      "0.20113449528644495 0.6896093935040454 1.0\n",
      "0.5102516369446016 -0.020506147698505558 0.9999999999999998\n",
      "0.5 0.0 1.0000000000000002\n",
      "0.5000000000000001 -2.2204460492503136e-16 1.0\n",
      "0.5 0.0 1.0\n",
      "0.5 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "clf_adaboost = AdaBoost(n_estimators=10)\n",
    "clf_adaboost.fit(X_train, y_train)\n",
    "#result = clf_adaboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(clf_adaboost.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Show the accuracy score of test data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
    "\n",
    "1. **n_estimators**: The number of trees in the forest. \n",
    "2. **max_features**: The number of random select features to consider when looking for the best split\n",
    "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, n_estimators, max_features, bootstrap=True, max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = int(max_features)\n",
    "        self.bootstrap = bootstrap\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def build_forest(self, X, y):\n",
    "        # train n_estimators trees\n",
    "        for i in range(self.n_estimators):\n",
    "            # random select attribute in training data to be the random vector\n",
    "            random_attributes = np.random.choice(X.columns, size=self.max_features, replace=False)\n",
    "            X_train = X[random_attributes]\n",
    "            y_train = y\n",
    "\n",
    "            # if bootstrap=True, random sample from training data to be the new training data\n",
    "            if self.bootstrap:\n",
    "                X_train = X_train.sample(frac=0.7, replace=False)\n",
    "                y_train = y_train[X_train.index]\n",
    "            \n",
    "            # train decision tree\n",
    "            tree = DecisionTree(criterion='gini', max_depth=self.max_depth)\n",
    "            tree.fit(X_train, y_train)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        voting_results = []\n",
    "        \n",
    "        # get prediction from each tree in random forest\n",
    "        for tree in self.trees:\n",
    "            prediction = tree.predict(X)\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        # use majority votes to get the final prediction\n",
    "        predictions = np.array(predictions)\n",
    "        for i in range(len(X)):\n",
    "            value, counts = np.unique(predictions[:, i], return_counts=True)\n",
    "            voting = value[np.argmax(counts)]\n",
    "            voting_results.append(voting)\n",
    "\n",
    "        return np.array(voting_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(len(X_train.columns)))\n",
    "clf_10tree.build_forest(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion=gini, max_depth=3, accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "print(\"criterion=gini, max_depth=3, accuracy:\", accuracy_score(clf_10tree.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1\n",
    "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of test data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(len(X_train.columns)), max_depth=10)\n",
    "clf_100tree = RandomForest(n_estimators=100, max_features=np.sqrt(len(X_train.columns)), max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=10, max_features=sqrt(n_features), accuracy: 0.81\n",
      "n_estimators=100, max_features=sqrt(n_features), accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "clf_10tree.build_forest(X_train, y_train)\n",
    "print(\"n_estimators=10, max_features=sqrt(n_features), accuracy:\", accuracy_score(clf_10tree.predict(X_test), y_test))\n",
    "\n",
    "clf_100tree.build_forest(X_train, y_train)\n",
    "print(\"n_estimators=100, max_features=sqrt(n_features), accuracy:\", accuracy_score(clf_100tree.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2\n",
    "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of test data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(len(X_train.columns)), max_depth=10)\n",
    "clf_all_features = RandomForest(n_estimators=10, max_features=len(X_train.columns), max_depth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=10, max_features=sqrt(n_features), accuracy: 0.79\n",
      "n_estimators=10, max_features=n_features, accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "clf_random_features.build_forest(X_train, y_train)\n",
    "print(\"n_estimators=10, max_features=sqrt(n_features), accuracy:\", accuracy_score(clf_random_features.predict(X_test), y_test))\n",
    "\n",
    "clf_all_features.build_forest(X_train, y_train)\n",
    "print(\"n_estimators=10, max_features=n_features, accuracy:\", accuracy_score(clf_all_features.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.\n",
    "Try you best to get highest test accuracy score by \n",
    "- Feature engineering\n",
    "- Hyperparameter tuning\n",
    "- Implement any other ensemble methods, such as gradient boosting. Please note that you cannot call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'your_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4130/3429442998.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myour_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'your_model' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = your_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary\n",
    "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
