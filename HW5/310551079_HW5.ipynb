{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "# neural network modules\n",
    "import torch.nn as nn\n",
    "# optimizers\n",
    "import torch.optim as optim\n",
    "# transformations \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset \n",
    "https://drive.google.com/drive/folders/1vI8Bkk5DojitLNkpz-UT30jOEay0XXon?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = np.load(\"x_train.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "y_train = y_train.squeeze()\n",
    "\n",
    "x_test = np.load(\"x_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# It's a multi-class classification problem \n",
    "class_index = {'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4,\n",
    "               'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n",
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypterparameters\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "lr = 0.005\n",
    "\n",
    "# calculate the mean and std of the cifar10 dataset\n",
    "def get_mean_and_std(x_train):\n",
    "    mean = []\n",
    "    std = []\n",
    "    for i in range(3):\n",
    "        mean.append(np.mean((x_train/255)[:, :, :, i]))\n",
    "        std.append(np.std((x_train/255)[:, :, :, i]))\n",
    "\n",
    "    mean = np.array(mean)\n",
    "    std = np.array(std)\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "class Cifar10(Dataset):\n",
    "    def __init__(self, data, label, transform=None):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# data augmentation with resize and normalization (project data to [-1, 1])\n",
    "mean, std = get_mean_and_std(x_train)\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),  \n",
    "        transforms.ToTensor(),        \n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "train_dataset = Cifar10(x_train, y_train, transform=transform)\n",
    "test_dataset = Cifar10(x_test, y_test, transform=transform)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_dataloader)\n",
    "images, labels = dataiter.next()\n",
    "assert images.shape[0] == batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Reference\n",
    "[DenseNet (CVPR 2017)](https://arxiv.org/pdf/1608.06993.pdf)  \n",
    "[FINETUNING TORCHVISION MODELS](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ FINETUNING ##################################\n",
    "\n",
    "# Load a pretrained model and reset final fully connected layer\n",
    "\n",
    "# load a pretrained model (densenet121)\n",
    "model = models.densenet121(pretrained=True)\n",
    "\n",
    "# reset final fully connected layer (num_ftrs = 1024)\n",
    "num_ftrs = model.classifier.in_features\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "                        nn.Linear(num_ftrs, 256),  \n",
    "                        nn.ReLU(), \n",
    "                        nn.Dropout(0.2),\n",
    "                        nn.Linear(256, 10))\n",
    "\n",
    "# copy weights for futher retraining on full train dataset\n",
    "model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "# move model to a device\n",
    "model = model.to(device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# all parameters are being optimized\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, num_epochs, dataloader_train):\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # set model to the training mode\n",
    "        model.train()\n",
    "\n",
    "        n_samples = 0\n",
    "        correct_train = 0\n",
    "        epoch_loss_train = 0\n",
    "\n",
    "        loop = tqdm(enumerate(dataloader_train), total=len(dataloader_train))\n",
    "        for i, (images, labels) in loop:\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward pass (pred)\n",
    "            pred = model(images)\n",
    "\n",
    "            # loss\n",
    "            loss = criterion(pred, labels)\n",
    "\n",
    "            # mean loss * num samples in batch\n",
    "            epoch_loss_train += labels.shape[0] * loss.item()\n",
    "\n",
    "            # empty gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # gradient (backpropagation)\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # values, indexes\n",
    "            value, predicted = torch.max(pred, 1)\n",
    "\n",
    "            # += batch_size\n",
    "            n_samples += labels.shape[0]\n",
    "            # num of correctly predicted in this batch\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            loop.set_description(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "            loop.set_postfix(loss=epoch_loss_train / n_samples, acc=f'{100 * correct_train / n_samples:.2f}%')\n",
    "\n",
    "        # train acc per epoch\n",
    "        train_acc = 100 * correct_train / n_samples\n",
    "        # train loss per epoch\n",
    "        epoch_loss_train = epoch_loss_train / n_samples\n",
    "\n",
    "        print(f'Train accuracy: {train_acc:.2f}, loss: {epoch_loss_train:.2f}')\n",
    "\n",
    "        # find best accuracy on training data\n",
    "        if train_acc > best_acc:\n",
    "            best_acc = train_acc\n",
    "            # copy current model weights\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print('=' * 80)\n",
    "\n",
    "    print('Best train acc: {:2f}'.format(best_acc))\n",
    "\n",
    "    # load best weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]: 100%|██████████| 782/782 [03:58<00:00,  3.28it/s, acc=76.35%, loss=0.86] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 76.35, loss: 0.86\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/5]: 100%|██████████| 782/782 [03:42<00:00,  3.51it/s, acc=93.07%, loss=0.22] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 93.07, loss: 0.22\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/5]: 100%|██████████| 782/782 [03:49<00:00,  3.40it/s, acc=95.92%, loss=0.131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 95.92, loss: 0.13\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/5]: 100%|██████████| 782/782 [03:49<00:00,  3.41it/s, acc=97.85%, loss=0.077] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 97.85, loss: 0.08\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/5]: 100%|██████████| 782/782 [03:43<00:00,  3.51it/s, acc=99.03%, loss=0.0429]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 99.03, loss: 0.04\n",
      "================================================================================\n",
      "Best train acc: 99.030000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = train(model, criterion, optimizer, num_epochs, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### PREDICT ####################################\n",
    "\n",
    "def model_predict(model, test_dataloader):\n",
    "    predictions = np.array([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for images, _ in test_dataloader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            # value, index\n",
    "            v, pred = torch.max(outputs, 1)\n",
    "\n",
    "            pred = pred.cpu().numpy()\n",
    "\n",
    "            predictions = np.concatenate((predictions, pred), axis=None)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), 'HW5_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://img-blog.csdnimg.cn/20190623084800880.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lqcDE5ODcxMDEz,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DO NOT MODIFY CODE BELOW!\n",
    "**Please screen shot your results and post it on your report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_predict(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_pred.shape == (10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of my model on test set:  0.9562\n"
     ]
    }
   ],
   "source": [
    "y_test = np.load(\"y_test.npy\")\n",
    "print(\"Accuracy of my model on test set: \", accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('PRML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8906f8bfc9964f721feb47cef4f06cf014afc885549639fc8219e4e408683c79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
